# -*- coding: utf-8 -*-
"""3-4_2_Preprocessing_Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BTYJ1q4cKgMs4oPxUwUhDWI8AiZHVwho
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install xgboost

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.model_selection import train_test_split

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB  # For continuous features
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler, OneHotEncoder

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
import joblib

import sys
import json

# Load thenj model
model = joblib.load("logreg_model.pkl")

def predict_heart_disease(input_data):
    model = joblib.load("model.pkl")
    df = pd.DataFrame([input_data])
    prediction = model.predict(df)[0]
    return "High risk" if prediction == 1 else "Low risk"

if __name__ == "__main__":
    try:
        json_input = sys.argv[1]
        input_data = json.loads(json_input)
        result = predict_heart_disease(input_data)
        print(result)
    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)


df = pd.read_csv('3-Senior_Apu_heart.csv')
# df = pd.read_csv('3-Senior_Apu_heart.csv')
# df = df.drop(columns=['id'])  # Drop the 'id' column
df.head()

num_rows, num_cols = df.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

# Drop duplicates
df.drop_duplicates(inplace=True)

# Check for constant columns
df.nunique()

num_rows, num_cols = df.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

# Check for null values
null_values = df.isnull().sum()
print("Null values in each column:\n", null_values)
#df.fillna(df.median(), inplace=True)  # Fill missing numerical values with median

# Find rows with any negative value
negative_rows = df[(df < 0).any(axis=1)]

# Remove rows with negative values
df = df[~(df < 0).any(axis=1)]

num_rows, num_cols = df.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

for column in df.columns:
    min_value = df[column].min()
    max_value = df[column].max()
    print(f"Column: {column}")
    print(f"  Minimum: {min_value}")
    print(f"  Maximum: {max_value}")
    print("-" * 20)  # Add a separator for better readability

# Function to detect outliers
def detect_outliers(df, column, low, up):

    # Identify outliers
    outliers = df[(df[column] < low) | (df[column] > up)].index.tolist()

    return outliers

# Check for outliers in specific column
all_outliers = []

outliers_age = detect_outliers(df, 'age', 10, 100)
print(f"{len(outliers_age)} Outliers in age:\n", outliers_age)
all_outliers.extend(outliers_age)

outliers_sex = detect_outliers(df, 'sex', 0, 1)
print(f"{len(outliers_sex)} Outliers in sex:\n", outliers_sex)
all_outliers.extend(outliers_sex)

outliers_bmi = detect_outliers(df, 'bmi', 14, 55)
print(f"{len(outliers_bmi)} Outliers in bmi:\n", outliers_bmi)
all_outliers.extend(outliers_bmi)

outliers_cp = detect_outliers(df, 'cp', 0, 2)
print(f"{len(outliers_cp)} Outliers in cp:\n", outliers_cp)
all_outliers.extend(outliers_cp)

outliers_trestbps = detect_outliers(df, 'trestbps', 90, 200)
print(f"{len(outliers_trestbps)} Outliers in resting bp s:\n", outliers_trestbps)
all_outliers.extend(outliers_trestbps)

outliers_chol = detect_outliers(df, 'chol', 50, 600)
print(f"{len(outliers_chol)} Outliers in chol:\n", outliers_chol)
all_outliers.extend(outliers_chol)

outliers_fbs = detect_outliers(df, 'fbs', 0, 1)
print(f"{len(outliers_fbs)} Outliers in fasting blood sugar:\n", outliers_fbs)
all_outliers.extend(outliers_fbs)

outliers_diabetes = detect_outliers(df, 'diabetes', 0, 1)
print(f"{len(outliers_diabetes)} Outliers in resting ecg:\n", outliers_diabetes)
all_outliers.extend(outliers_diabetes)

outliers_maxHR = detect_outliers(df, 'maxHR', 60, 200)
print(f"{len(outliers_maxHR)} Outliers in max heart rate:\n", outliers_maxHR)
all_outliers.extend(outliers_maxHR)

outliers_smoker = detect_outliers(df, 'smoker', 0, 2)
print(f"{len(outliers_smoker)} Outliers in aexercise angina:\n", outliers_smoker)
all_outliers.extend(outliers_smoker)

# Drop the outlier rows
df_cleaned = df.drop(all_outliers)

def detect_all_outliers_iqr(df):
    """
    Detects outliers in all specified numerical columns of a Pandas DataFrame
    using the IQR method.

    Args:
        data: Pandas DataFrame containing the data.
        numerical_cols: A list of column names representing the numerical features.

    Returns:
        A list of all outlier indices in the DataFrame.
    """
    all_outliers = []
    numerical_cols = ['ap_hi', 'ap_lo']
    for col in numerical_cols:
        # Calculate quartiles
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        # Calculate upper and lower bounds
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify outliers for the current column
        outliers_col = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()

        # Print information about outliers in the current column
        print(f"{len(outliers_col)} Outliers in {col}:\n", outliers_col)

        # Extend the list of all outliers
        all_outliers.extend(outliers_col)

    # Remove duplicates from the list of all outliers
    all_outliers = list(set(all_outliers))

    # Drop the outlier rows
    df_cleaned = df.drop(all_outliers)

    return df_cleaned

# For 'ap_hi', 'ap_lo' only
# df_cleaned = detect_all_outliers_iqr(df_cleaned)

num_rows, num_cols = df_cleaned.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")

# Check for constant columns
df_cleaned.nunique()

# Calculate BMI and add it as a new column
# df_cleaned['bmi'] = df_cleaned['weight'] / ((df_cleaned['height'] / 100) ** 2)  # height in cm, weight in kg

# Calculate MAP and add it as a new column
# df_cleaned['map'] = (2 * df_cleaned['ap_lo'] + df_cleaned['ap_hi']) / 3

df_cleaned.head()

# df_cleaned = df_cleaned.drop(['height', 'weight', 'ap_lo', 'ap_hi'], axis=1)

# column_names = list(df_cleaned.columns)
# column_names.insert(1, column_names[8])  # Insert 'bmi' at 2rd position
# column_names.insert(2, column_names[10]) # Insert 'map' at 3rd position
# column_names = column_names[:-2] #remove tha last two columns
# df_cleaned = df_cleaned[column_names]

# df_cleaned.head()

# Feature Selection
plt.figure(figsize=(12, 10))
sns.heatmap(df_cleaned.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

def correlation(df, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = df.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(df_cleaned, 0.7)
print(f"{len(corr_features)} Correlated features detected:\n", corr_features)

df_cleaned.head()

# Apply normalization using StandardScaler
numerical_features = ['age', 'bmi', 'trestbps', 'chol', 'maxHR']
categorical_features = ['sex', 'cp', 'fbs', 'diabetes', 'smoker', 'target']
scaler = StandardScaler()
df_num = pd.DataFrame(scaler.fit_transform(df_cleaned[numerical_features]), columns=numerical_features, index=df_cleaned.index)
df_cat = pd.DataFrame(df_cleaned[categorical_features], columns=categorical_features, index=df_cleaned.index)
df_processed = pd.concat([df_num, df_cat], axis=1)
# df_cleaned[column_names] = scaler.fit_transform(df_cleaned[column_names])

df_processed.head()

# Preprocess Data
X = df_processed.drop('target', axis=1)
y = df_processed['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Top features
selector = SelectKBest(score_func=f_classif, k='all')
X_new = selector.fit_transform(X, y)
# SCORE_COL = pd.DataFrame(selector.scores_,columns = ['score value'])
# PVALUE_COL = pd.DataFrame(selector.pvalues_,columns = ['p_value'])

print("Top features:")
# Get scores and selected feature names
scores = selector.scores_
selected_mask = selector.get_support()
selected_features = X.columns[selected_mask]
selected_scores = scores[selected_mask]

# Print feature names with their scores
for feature, score in zip(selected_features, selected_scores):
    print(f"{feature}: {score:.4f}")

df_processed = df_processed.drop(['maxHR', 'fbs'], axis=1)

# Check for constant columns
df_processed.nunique()

# Encode categorical features
numerical_features = ['age', 'bmi', 'trestbps', 'chol']
categorical_features = ['sex', 'cp', 'diabetes', 'smoker']
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # sparse=False for DataFrame output
df_cat = pd.DataFrame(encoder.fit_transform(df_processed[categorical_features]), columns=encoder.get_feature_names_out(categorical_features), index=df_processed.index)
df_num = pd.DataFrame(df_processed[numerical_features], columns=numerical_features, index=df_processed.index)
df_processed_encode = pd.concat([df_num, df_cat], axis=1)

df_processed_encode.head()

df_processed_encode.columns

# Preprocess Data
X = df_processed_encode
y = df_processed['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model with SVM Model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

"""# Take user input and get prediction"""

age = float(input("Enter age:"))
bmi = float(input("Enter bmi:"))
trestbps = float(input("Enter trestbps:"))
chol = float(input("Enter chol:"))
sex = int(input("Enter sex:"))
cp = int(input("Enter cp:"))
diabetes = int(input("Enter diabetes:"))
smoker = int(input("Enter smoker:"))

if sex == 1:
    sex_0 = 0
    sex_1 = 1
else:
    sex_0 = 1
    sex_1 = 0

if cp == 1:
    cp_0 = 0
    cp_1 = 0
    cp_2 = 1
elif cp == 2:
    cp_0 = 0
    cp_1 = 1
    cp_2 = 0
elif cp == 3:
    cp_0 = 1
    cp_1 = 0
    cp_2 = 0
else:
    cp_0 = 0
    cp_1 = 0
    cp_2 = 0

if diabetes == 1:
    diabetes_0 = 0
    diabetes_1 = 1
else:
    diabetes_0 = 1
    diabetes_1 = 0

if smoker == 1:
    smoker_0 = 0
    smoker_1 = 0
    smoker_2 = 1
elif smoker == 2:
    smoker_0 = 0
    smoker_1 = 1
    smoker_2 = 0
else:
    smoker_0 = 1
    smoker_1 = 0
    smoker_2 = 0

# Create a list with the specific row data
# specific_row_data = [-0.251412, 0.969874, -0.352114, -0.676635, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
specific_row_data = [age, bmi, trestbps, chol, sex_0, sex_1, cp_0, cp_1, cp_2, diabetes_0, diabetes_1, smoker_0, smoker_1, smoker_2]

# Convert the list to a format the model expects (a 2D array)
# We use pandas DataFrame here to match the format of X_train that the model was trained on
specific_row_df = pd.DataFrame([specific_row_data], columns=['age', 'bmi', 'trestbps', 'chol', 'sex_0', 'sex_1', 'cp_0', 'cp_1', 'cp_2', 'diabetes_0', 'diabetes_1', 'smoker_0', 'smoker_1', 'smoker_2'])

# Get the prediction using the trained svm_model
prediction = svm_model.predict(specific_row_df)

# Print the prediction
print(f"The prediction for the specific row is: {prediction[0]}")

# Train the model with RandomForest Model
RF_model = RandomForestClassifier(n_estimators=100, random_state=42)
RF_model.fit(X_train, y_train)

# Train the model with KNN Model
knn_model = KNeighborsClassifier(n_neighbors=5)  # (number of neighbors to consider)
knn_model.fit(X_train, y_train)

# Train the model with Decision Tree Model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Train the model with Logistic Regression Model
logreg_model = LogisticRegression(random_state=42)
logreg_model.fit(X_train, y_train)

# Train the model with Naive Bayes Model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Train the model with MLP Model
mlp_model = MLPClassifier(hidden_layer_sizes=(100), random_state=42, max_iter=300)
mlp_model.fit(X_train, y_train)

# Train the model with  XGBoost Model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

# Make Predictions and Evaluate
def predict_and_evaluate(models):
    for model_name, model in models.items():
        y_pred = model.predict(X_test)  # Predict on the test set

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        print(f"Metrics for {model_name}:")
        print(f"  Accuracy: {accuracy}")
        print(f"  Precision: {precision}")
        print(f"  Recall: {recall}")
        print(f"  F1 Score: {f1}")
        print("-" * 20)  # Separator for better readability

models = {
    "SVM": svm_model,
    "RF": RF_model,
    "KNN": knn_model,
    "DT": dt_model,
    "LR": logreg_model,
    "Naive Bayes": nb_model,
    "MLP": mlp_model,
    "XGBoost": xgb_model
}

predict_and_evaluate(models)

param_distributions = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

svm = SVC()
svm_best = RandomizedSearchCV(estimator=svm, param_distributions=param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42)
svm_best.fit(X_train, y_train)
# best_svm = grid_search.best_estimator_
params_best = svm_best.best_params_

print("Best SVM Hyperparameters:", params_best)

y_pred = svm_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf = RandomForestClassifier()
# Use RandomizedSearchCV with a smaller number of iterations:
rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=50, cv=3, scoring='accuracy', n_jobs=-1)
rf_random.fit(X_train, y_train)
# best_rf = grid_search.best_estimator_
params_random = rf_random.best_params_

print("Best RF Hyperparameters:", params_random)

y_pred = rf_random.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

param_distributions = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

knn = KNeighborsClassifier()
knn_best = RandomizedSearchCV(estimator=knn, param_distributions=param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42)  # Specify n_iter and random_state
knn_best.fit(X_train, y_train)
# best_knn = grid_search.best_estimator_
params_best = knn_best.best_params_

print("Best KNN Hyperparameters:", params_best)

y_pred = knn_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

param_distributions = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt = DecisionTreeClassifier()
dt_best = RandomizedSearchCV(estimator=dt, param_distributions=param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42)
dt_best.fit(X_train, y_train)
# best_dt = grid_search.best_estimator_
params_best = dt_best.best_params_

print("Best DT Hyperparameters:", params_best)

y_pred = dt_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

param_distributions = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear', 'lbfgs', 'sag', 'saga']
}

lr = LogisticRegression()
lr_best = RandomizedSearchCV(estimator=lr, param_distributions=param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42)
lr_best.fit(X_train, y_train)
# best_lr = grid_search.best_estimator_
params_best = lr_best.best_params_

print("Best LR Hyperparameters:", params_best)

y_pred = lr_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

param_distributions = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]
}

nb = GaussianNB()
nb_best = RandomizedSearchCV(estimator=nb, param_distributions=param_distributions, n_iter=4, cv=5, scoring='accuracy', random_state=42)
nb_best.fit(X_train, y_train)
# best_nb = grid_search.best_estimator_
params_best = nb_best.best_params_

print("Best NB Hyperparameters:", params_best)

y_pred = nb_best.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

# param_grid = {
#     'hidden_layer_sizes': [(50,), (100,), (50, 50)],
#     'activation': ['relu', 'tanh', 'logistic'],
#     'solver': ['adam', 'sgd'],
#     'alpha': [0.0001, 0.001, 0.01]
# }

# mlp = MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=500)
# mlp_best = RandomizedSearchCV(estimator=mlp, param_distributions=param_grid, cv=5, scoring='accuracy')
# mlp_best.fit(X_train, y_train)
# # best_mlp = grid_search.best_estimator_
# params_best = mlp_best.best_params_

# print("Best MLP Hyperparameters:", params_best)

# y_pred = mlp_best.predict(X_test)
# accuracy = accuracy_score(y_test, y_pred)
# print(f"  Accuracy: {accuracy}")

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

xgb = XGBClassifier()
xgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid, cv=5, scoring='accuracy')
xgb_random.fit(X_train, y_train)
# best_xgb = grid_search.best_estimator_
params_best = xgb_random.best_params_

print("Best XGB Hyperparameters:", params_best)

y_pred = xgb_random.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

# print("Classification Report:\n", classification_report(y_test, y_pred))

models = {
    "SVM": svm_best,
    "RF": rf_random,
    "KNN": knn_best,
    "DT": dt_best,
    "LR": lr_best,
    "Naive Bayes": nb_best,
    # "MLP": mlp_model,
    "XGBoost": xgb_random
}

print("### After Hyperparameter tuning:")
predict_and_evaluate(models)

"""# Store & Load Model"""

model_filename_lr = 'logreg_model.pkl'
joblib.dump(lr_best, model_filename_lr)

# Load a model from the file
loaded_lr_model = joblib.load('logreg_model.pkl')

# Now you can use the loaded model for predictions
# For example:
y_pred = loaded_lr_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"  Accuracy: {accuracy}")

"""# ANN"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  # .values to get NumPy array
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

# Create datasets and data loaders
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class ANN(nn.Module):
    def __init__(self, input_size):
        super(ANN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, 32)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(0.2)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.dropout1(x)
        x = self.fc2(x)
        x = self.relu2(x)
        x = self.dropout2(x)
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

input_size = X_train.shape[1]
model = ANN(input_size)

optimizer = optim.Adam(model.parameters())
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss

# Train the Model
num_epochs = 50

for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()  # Reset gradients
        output = model(data)  # Forward pass
        loss = criterion(output, target.unsqueeze(1))  # Calculate loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Update weights

def predict_and_evaluate_ANN(models):
    for model_name, model in models.items():
        if isinstance(model, nn.Module):  # Check if it's a PyTorch model
            model.eval()  # Set to evaluation mode
            with torch.no_grad():
                y_pred = []
                for data, _ in test_loader:
                    output = model(data)
                    predicted = (output > 0.5).float()
                    y_pred.extend(predicted.squeeze().tolist())
            y_pred = torch.tensor(y_pred)
        else:  # Assume it's a scikit-learn model
            y_pred = model.predict(X_test)

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        print(f"Metrics for {model_name}:")
        print(f"  Accuracy: {accuracy}")
        print(f"  Precision: {precision}")
        print(f"  Recall: {recall}")
        print(f"  F1 Score: {f1}")
        print("-" * 20)

model.eval()  # Set model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    correct = 0
    total = 0
    for data, target in test_loader:
        output = model(data)
        predicted = (output > 0.5).float()  # Threshold for binary classification
        total += target.size(0)
        correct += (predicted == target.unsqueeze(1)).sum().item()

    accuracy = 100 * correct / total
    print(f'Test Accuracy: {accuracy:.2f}%')

    predict_and_evaluate_ANN({"ANN": model})